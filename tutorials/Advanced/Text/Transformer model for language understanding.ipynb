{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q tfds-nightly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "one_pt,one_en=next(iter(train_examples))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_datasets' has no attribute 'deprecated'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-76-bcab781f0508>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# 这个的功能除了分词，当遇到oov的时候相当于把词做切分了，比如Transformer就可能变成Trans和fromer两个词\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n\u001B[0m\u001B[1;32m      5\u001B[0m     (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow_datasets' has no attribute 'deprecated'"
     ]
    }
   ],
   "source": [
    "# 这个类库不知道为啥我用不了，我换成tokenizer了\n",
    "# 这个的功能除了分词，当遇到oov的时候相当于把词做切分了，比如Transformer就可能变成Trans和fromer两个词\n",
    "# The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# 只保留2**13个词 八千多\n",
    "tokenizer_en=tf.keras.preprocessing.text.Tokenizer(num_words=2**13,oov_token=\"<UNK>\")\n",
    "tokenizer_en.fit_on_texts([en.numpy().decode('utf-8') for pt, en in train_examples])\n",
    "tokenizer_pt=tf.keras.preprocessing.text.Tokenizer(num_words=2**13,oov_token=\"<UNK>\")\n",
    "tokenizer_pt.fit_on_texts([pt.numpy().decode('utf-8') for pt, en in train_examples])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [[1, 13, 1694]]\n",
      "The original string: ['<UNK> is awesome']\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.texts_to_sequences([sample_string])\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.sequences_to_texts(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "([8192, 5, 35, 1, 2, 761, 3901, 2, 358, 2257, 18, 1702, 4, 8, 2, 1, 8193],\n [8192,\n  3,\n  53,\n  12,\n  1085,\n  1,\n  12,\n  86,\n  117,\n  237,\n  2,\n  39,\n  2009,\n  5,\n  2286,\n  84,\n  13,\n  1,\n  8193])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  # 与官网不同，我做了修改，使用的是tokenizer\n",
    "  lang1 = [2**13] + tokenizer_pt.texts_to_sequences(\n",
    "      [lang1.numpy().decode(\"utf8\")])[0] + [2**13+1]\n",
    "\n",
    "  lang2 = [2**13] + tokenizer_en.texts_to_sequences(\n",
    "      [lang2.numpy().decode(\"utf8\")])[0] + [2**13+1]\n",
    "\n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\n",
    "encode(one_pt,one_en)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(64, 39), dtype=int64, numpy=\n array([[8192,  282, 7964, ...,    0,    0,    0],\n        [8192,   87,  136, ...,    0,    0,    0],\n        [8192, 4239,    9, ...,    0,    0,    0],\n        ...,\n        [8192,   87,    2, ...,    0,    0,    0],\n        [8192,   26, 7434, ...,    0,    0,    0],\n        [8192,   21, 1247, ...,    0,    0,    0]])>,\n <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n array([[8192,   85,   18, ...,    0,    0,    0],\n        [8192,    8,   19, ...,    0,    0,    0],\n        [8192,    8, 2251, ...,    0,    0,    0],\n        ...,\n        [8192,    9,   19, ...,    0,    0,    0],\n        [8192,    8, 1562, ...,    0,    0,    0],\n        [8192,  941,   10, ...,    0,    0,    0]])>)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(int,\n            {'e': 17266,\n             'procura': 105,\n             'tiramos': 15,\n             'é': 12290,\n             'a': 20017,\n             'melhoramos': 2,\n             'vantagem': 31,\n             'que': 19106,\n             'única': 229,\n             'serendipidade': 1,\n             'quando': 2285,\n             'impressão': 40,\n             'da': 5169,\n             'ativos': 17,\n             'estes': 711,\n             'fossem': 65,\n             'mas': 4967,\n             'se': 6894,\n             'fatores': 29,\n             'não': 7902,\n             'me': 2390,\n             'tinham': 281,\n             'eles': 1413,\n             'de': 18808,\n             'testar': 44,\n             'curiosidade': 32,\n             'como': 4715,\n             'esta': 1904,\n             'razão': 265,\n             'agnóstica': 1,\n             'ainda': 801,\n             'ter': 1195,\n             'consciente': 23,\n             'rebeldia': 2,\n             'posso': 264,\n             'qual': 470,\n             'eu': 3772,\n             'pela': 666,\n             'fé': 42,\n             \"''\": 4096,\n             'usar': 437,\n             'mesa': 54,\n             'podem': 1219,\n             'tudo': 960,\n             'corpo': 247,\n             'no': 4472,\n             'sobre': 1547,\n             'meu': 1259,\n             'muito': 2852,\n             'produtos': 89,\n             'teatro': 22,\n             'pessoas': 2462,\n             'são': 2629,\n             'escrevo': 16,\n             'segurança': 165,\n             'as': 5363,\n             'acerca': 224,\n             'seguras': 21,\n             'realidade': 313,\n             'do': 5876,\n             'nada': 388,\n             'sentirem': 19,\n             'fazem': 312,\n             'na': 4218,\n             'resultados': 117,\n             'os': 6288,\n             'anunciaram': 5,\n             'sensíveis': 9,\n             'últimos': 150,\n             'duma': 79,\n             'mina': 7,\n             'ferro': 15,\n             'dois': 611,\n             'bem': 1317,\n             'colocaram': 13,\n             'fundo': 116,\n             'minnesota': 2,\n             'dias': 366,\n             'mais': 4102,\n             'até': 1009,\n             'agora': 1441,\n             'nos': 2980,\n             'têm': 1044,\n             'medo': 189,\n             'algumas': 470,\n             'gostem': 8,\n             'delas': 149,\n             'fixado': 2,\n             'o': 15550,\n             'aconteceu': 235,\n             'fora': 301,\n             'está': 2024,\n             'chris': 54,\n             'preço': 75,\n             'margem': 17,\n             'poder': 341,\n             'pergunta': 186,\n             'porque': 2172,\n             'fiquei': 109,\n             'à': 1991,\n             'minha': 1322,\n             'volta': 372,\n             'sem': 643,\n             'pensei': 156,\n             'lógica': 28,\n             'autor': 37,\n             'uma': 8180,\n             'subjacente': 9,\n             'proteção': 44,\n             'direitos': 107,\n             'descobri': 70,\n             'será': 323,\n             'indústrias': 24,\n             'série': 124,\n             'havia': 303,\n             'um': 8745,\n             'também': 1472,\n             'apurou': 2,\n             'vencedor': 12,\n             'segunda': 155,\n             'ronda': 1,\n             'votos': 12,\n             'claro': 425,\n             'sucesso': 146,\n             'foi': 1976,\n             'cirurgia': 61,\n             'criados': 15,\n             'vídeo': 223,\n             'visão': 152,\n             'nova': 440,\n             'vez': 977,\n             'virámo': 2,\n             'em': 6496,\n             'geração': 84,\n             'uso': 96,\n             'noturna': 2,\n             'óculos': 10,\n             'para': 7980,\n             'inicialmente': 20,\n             'sensores': 17,\n             'disso': 356,\n             'olá': 56,\n             'sou': 414,\n             'nome': 156,\n             'marcin': 1,\n             'agricultor': 10,\n             'tecnólogo': 1,\n             'coisas': 1178,\n             'precioso': 8,\n             'conte': 3,\n             'ferramentas': 124,\n             'fácil': 211,\n             'talvez': 539,\n             'somente': 43,\n             'contar': 207,\n             'caixa': 88,\n             'arranjar': 43,\n             'realmente': 668,\n             'seja': 428,\n             'altura': 289,\n             'intangíveis': 4,\n             'tangível': 7,\n             'vida': 1055,\n             '6000': 7,\n             'ser': 2170,\n             'prevê': 8,\n             'além': 204,\n             'século': 152,\n             '—': 1363,\n             'cerca': 380,\n             'quase': 370,\n             'línguas': 55,\n             'todas': 672,\n             'final': 142,\n             'existem': 291,\n             'faladas': 1,\n             'deixarão': 2,\n             'amada': 7,\n             'era': 1716,\n             'textura': 9,\n             'decidir': 36,\n             'associada': 15,\n             'cérebro': 352,\n             'recompensa': 26,\n             'só': 940,\n             'fazer': 2181,\n             'limpar': 20,\n             'terra': 293,\n             'mesmo': 1296,\n             'curve': 1,\n             'hectare': 1,\n             '150': 37,\n             'ervas': 9,\n             'pequeno': 259,\n             '000': 246,\n             'arroz': 18,\n             'trabalhe': 5,\n             'vezes': 645,\n             'plantar': 23,\n             'crescer': 98,\n             'pedir': 93,\n             'colheita': 5,\n             'acredito': 128,\n             'pode': 1150,\n             'potencialmente': 22,\n             'podemos': 1173,\n             'com': 5282,\n             'nanopatch': 20,\n             'ajudá': 17,\n             'temos': 1518,\n             'lo': 588,\n             'acontecer': 327,\n             'alavanca': 7,\n             'complexa': 29,\n             'importante': 469,\n             'aqui': 1424,\n             'sistema': 473,\n             'máquina': 106,\n             'questão': 289,\n             'neste': 494,\n             'pessoa': 397,\n             'grande': 717,\n             'asilo': 7,\n             'refugiados': 27,\n             'teoria': 95,\n             'direito': 112,\n             'adn': 142,\n             'espaço': 310,\n             'análises': 16,\n             'falemos': 12,\n             'companhia': 44,\n             'durante': 545,\n             'sentido': 191,\n             'esse': 556,\n             'isso': 2981,\n             'dentro': 353,\n             'período': 72,\n             'hora': 136,\n             'ao': 2145,\n             'upgrade': 1,\n             'chamado': 194,\n             'comecei': 218,\n             'há': 2076,\n             'tempo': 996,\n             'projecto': 78,\n             'green': 8,\n             'accounting': 1,\n             'verde': 68,\n             'contabilidade': 9,\n             'project': 7,\n             'valor': 152,\n             'pelos': 197,\n             'participantes': 16,\n             'uns': 276,\n             'seu': 853,\n             'outros': 584,\n             'criado': 38,\n             'portanto': 691,\n             'mudança': 198,\n             'climática': 34,\n             'preocupem': 13,\n             'afinal': 55,\n             'amor': 113,\n             'algoritmo': 25,\n             'melhor': 616,\n             'países': 275,\n             'pão': 54,\n             'branco': 65,\n             'quanto': 330,\n             'for': 120,\n             'muitos': 522,\n             'criminalizar': 2,\n             'mental': 79,\n             'doença': 171,\n             'igualmente': 29,\n             'deixar': 167,\n             'disto': 152,\n             'quão': 118,\n             'avançados': 9,\n             'novamente': 102,\n             'dispositivos': 41,\n             'incrível': 137,\n             'tornaram': 58,\n             'vão': 388,\n             'tirar': 94,\n             'vou': 664,\n             'avaliá': 1,\n             'los': 307,\n             'zero': 57,\n             'disse': 677,\n             'remorso': 1,\n             'sobrevivente': 6,\n             'espécie': 212,\n             'tivesse': 91,\n             'sobrevivem': 3,\n             'cada': 726,\n             'diferentes': 409,\n             'nós': 1831,\n             'elas': 435,\n             'perecemos': 1,\n             'vidas': 202,\n             'significar': 15,\n             'muitas': 541,\n             'vai': 583,\n             'parar': 110,\n             'isto': 2779,\n             'ele': 1557,\n             'ideal': 21,\n             'tipo': 643,\n             'exactamente': 72,\n             'deste': 276,\n             'celebração': 7,\n             'criatividade': 54,\n             'obrigado': 499,\n             'estamos': 875,\n             'dinheiro': 296,\n             'gastar': 33,\n             'parece': 320,\n             'apareço': 3,\n             'rap': 3,\n             'sim': 361,\n             'canções': 27,\n             '1': 188,\n             '4': 99,\n             'logo': 91,\n             '6': 61,\n             'utilizámos': 9,\n             'género': 57,\n             'símbolos': 20,\n             'eram': 405,\n             'brand': 2,\n             'deixá': 11,\n             'desaparecer': 30,\n             'stewart': 8,\n             'anterior': 41,\n             'espécies': 96,\n             'animais': 147,\n             'massivamente': 3,\n             'extinção': 16,\n             'alterámos': 4,\n             'ecossistema': 28,\n             'deles': 250,\n             'fundamentais': 38,\n             'encontravam': 9,\n             'todo': 660,\n             'interferimos': 1,\n             'levar': 147,\n             'pronta': 12,\n             'estou': 541,\n             'família': 237,\n             'perfeita': 31,\n             'muita': 201,\n             'pressão': 50,\n             'daquela': 46,\n             'por': 4349,\n             'branca': 23,\n             'detrás': 46,\n             'tentar': 383,\n             'aplausos': 820,\n             'certa': 167,\n             'clássico': 16,\n             'incluídas': 1,\n             'nisto': 115,\n             'pensar': 528,\n             'pilhas': 7,\n             'maneira': 283,\n             'cotado': 1,\n             'bolsa': 13,\n             'possui': 15,\n             'atmosfera': 41,\n             'oh': 94,\n             'aceito': 9,\n             'recuando': 3,\n             'tem': 1331,\n             'trono': 2,\n             'anos': 1703,\n             'toda': 582,\n             'caso': 242,\n             'ano': 478,\n             'história': 599,\n             'rapaz': 59,\n             'pobre': 33,\n             'este': 1735,\n             '13': 63,\n             'asiático': 12,\n             '300': 50,\n             'país': 329,\n             'subiu': 13,\n             'sido': 314,\n             'c': 68,\n             'remoto': 12,\n             'notícia': 46,\n             'sódio': 2,\n             'imaginaria': 3,\n             'ali': 263,\n             'sítio': 90,\n             'ligadas': 20,\n             'building': 4,\n             'baixo': 176,\n             'água': 333,\n             'facto': 476,\n             'noite': 186,\n             'lâmpadas': 7,\n             'penso': 393,\n             'mangueiras': 2,\n             'luzes': 40,\n             'sabem': 385,\n             'rosa': 25,\n             'deus': 140,\n             'quem': 500,\n             'aquilo': 280,\n             'acenderam': 1,\n             'cortina': 8,\n             'lá': 565,\n             'ácido': 5,\n             'lançam': 3,\n             'tornar': 239,\n             'cor': 108,\n             'fotografia': 100,\n             'num': 919,\n             'woolworth': 1,\n             'pelo': 695,\n             'fumo': 14,\n             'escondido': 13,\n             'aspeto': 55,\n             'estão': 1337,\n             'vacina': 24,\n             'massa': 80,\n             'porém': 24,\n             'cara': 78,\n             'disponível': 47,\n             'índios': 2,\n             'índio': 1,\n             'existimos': 4,\n             'nacional': 81,\n             'semana': 134,\n             'amazónica': 3,\n             'fundação': 34,\n             'brasil': 45,\n             'grupos': 111,\n             'passada': 30,\n             'estive': 79,\n             'contactáveis': 1,\n             '110': 4,\n             'floresta': 78,\n             'físico': 76,\n             'deslocar': 8,\n             'segundos': 56,\n             'demorou': 19,\n             'terremoto': 3,\n             '60': 99,\n             'lugares': 103,\n             'nenhum': 136,\n             'quantos': 70,\n             'mudar': 325,\n             'u': 47,\n             'diferente': 348,\n             'ambientais': 13,\n             'assim': 1001,\n             'sua': 1022,\n             'continuar': 110,\n             'desafiar': 17,\n             'redor': 49,\n             'percepção': 21,\n             'sejam': 121,\n             'humanos': 233,\n             'algo': 845,\n             'ou': 2117,\n             'tecnológicos': 7,\n             'completamente': 207,\n             'recursos': 93,\n             'ver': 1350,\n             'formato': 18,\n             'interessada': 10,\n             'andar': 90,\n             'segredo': 32,\n             'lê': 20,\n             'ela': 796,\n             'vivem': 83,\n             'onde': 1029,\n             'nações': 39,\n             'aqueles': 162,\n             'sorte': 63,\n             'tarde': 180,\n             'terei': 6,\n             'és': 43,\n             'baixa': 69,\n             'abstenham': 1,\n             'favor': 79,\n             'dizer': 915,\n             'encontrar': 290,\n             'palco': 70,\n             'estava': 924,\n             'tecnologia': 387,\n             'outra': 587,\n             'pior': 90,\n             'época': 76,\n             'idosos': 48,\n             'cruel': 10,\n             'menos': 511,\n             'hoje': 758,\n             'começam': 77,\n             'jovens': 154,\n             'qualquer': 614,\n             'mudanças': 75,\n             'passado': 241,\n             'microfone': 20,\n             'enviar': 67,\n             'câmara': 79,\n             'forma': 1130,\n             'scratch': 19,\n             'mesma': 369,\n             'informação': 385,\n             'usa': 65,\n             'navegador': 10,\n             'tor': 5,\n             'energia': 312,\n             'mecânicos': 10,\n             'conseguimos': 212,\n             'edifícios': 88,\n             'independentes': 11,\n             'intenção': 13,\n             'nossa': 908,\n             'eficientes': 19,\n             'tornarmos': 16,\n             'termos': 176,\n             'evoluam': 1,\n             'tornando': 18,\n             'sistemas': 127,\n             'observar': 66,\n             'china': 130,\n             'poluição': 15,\n             'nesses': 34,\n             'corrupção': 21,\n             'acontece': 371,\n             'negro': 51,\n             'atrás': 215,\n             'buraco': 59,\n             'esconder': 25,\n             'manada': 6,\n             'colchetes': 1,\n             'virar': 29,\n             'seguir': 167,\n             'invertidos': 1,\n             'pareciam': 16,\n             'estaria': 29,\n             'interrompíamos': 1,\n             'esquina': 12,\n             'estivéssemos': 16,\n             'veículos': 31,\n             'pensamos': 132,\n             'páginas': 39,\n             'marte': 37,\n             'sinto': 85,\n             'sentir': 134,\n             'feliz': 75,\n             'maravilhado': 3,\n             'twitter': 49,\n             'recente': 47,\n             'adam': 12,\n             'maior': 527,\n             'tweet': 6,\n             'já': 899,\n             'viste': 4,\n             'primo': 29,\n             'olhei': 22,\n             'número': 324,\n             'compreensível': 4,\n             'técnica': 42,\n             'receios': 5,\n             'solução': 125,\n             'procurem': 11,\n             'engenheiros': 34,\n             'nuclear': 97,\n             'terão': 45,\n             'per': 16,\n             'aconteça': 31,\n             'capita': 17,\n             'acima': 69,\n             'vosso': 225,\n             'dólares': 229,\n             'rendimento': 41,\n             'democracia': 92,\n             'instabilidade': 4,\n             'novo': 341,\n             'depois': 1096,\n             'caiu': 22,\n             'década': 70,\n             'política': 160,\n             'fim': 239,\n             'desgoverno': 1,\n             'fui': 262,\n             'escola': 268,\n             'aprendi': 117,\n             'braille': 5,\n             'cinco': 219,\n             'tinha': 905,\n             'trabalho': 597,\n             'outras': 481,\n             'madre': 3,\n             'vanier': 3,\n             'jean': 11,\n             'objectivo': 33,\n             'teresa': 2,\n             'amiga': 25,\n             'falecida': 1,\n             'nunca': 535,\n             'calcutá': 4,\n             'diz': 315,\n             'mundo': 1362,\n             'próprios': 115,\n             'primeiro': 555,\n             'principalmente': 33,\n             'tentativas': 11,\n             'bali': 9,\n             'governo': 158,\n             'falharam': 7,\n             'primeiras': 49,\n             'convencer': 26,\n             'roldanas': 1,\n             'fios': 26,\n             'alguns': 596,\n             'molas': 1,\n             'começou': 164,\n             'ory': 1,\n             'vocês': 901,\n             'kenyan': 1,\n             'pundit': 1,\n             'tedtalk': 12,\n             'okolloh': 5,\n             'blog': 7,\n             'nairobi': 7,\n             'advogada': 5,\n             'site': 32,\n             'houve': 136,\n             'conhecer': 56,\n             'moçambique': 6,\n             'suécia': 17,\n             'lugar': 312,\n             'fazemos': 248,\n             'mente': 112,\n             'todos': 1564,\n             'contexto': 64,\n             'cordas': 28,\n             'mistério': 33,\n             'negra': 36,\n             'pontos': 123,\n             'vos': 983,\n             'três': 516,\n             'mantenham': 9,\n             'das': 1736,\n             'chave': 94,\n             'transmitido': 5,\n             'directo': 10,\n             'consumidores': 36,\n             'perspicazes': 1,\n             'obrigada': 250,\n             'religioso': 8,\n             'pensa': 59,\n             'modo': 268,\n             'desse': 97,\n             'feitos': 72,\n             'segundo': 232,\n             'ideia': 564,\n             'nosso': 798,\n             'interiores': 8,\n             'nossas': 464,\n             'som': 133,\n             'vamos': 725,\n             'ei': 38,\n             'eficaz': 45,\n             'apropriado': 12,\n             'desenhar': 41,\n             'ouvir': 143,\n             'divisões': 17,\n             'decoradores': 3,\n             'passar': 186,\n             'válido': 5,\n             'faz': 450,\n             'dar': 433,\n             'dá': 158,\n             'partir': 205,\n             'apenas': 1160,\n             'si': 230,\n             'prazer': 45,\n             'instrumentos': 32,\n             'escreve': 9,\n             'descobre': 9,\n             'swann': 4,\n             'poderia': 223,\n             'proust': 11,\n             'pessoal': 105,\n             'admitir': 17,\n             'preciso': 203,\n             'tortura': 5,\n             'daí': 54,\n             'exemplo': 585,\n             'próprio': 169,\n             'começa': 157,\n             'encanto': 3,\n             'verdade': 689,\n             'acréscimo': 2,\n             'colecção': 12,\n             'ciumentos': 1,\n             'instante': 23,\n             'particularmente': 59,\n             'amante': 5,\n             'tão': 709,\n             'escandinavos': 1,\n             'bons': 93,\n             'serem': 140,\n             'legado': 14,\n             'homogéneos': 1,\n             'pequenos': 140,\n             'escolher': 65,\n             'opções': 40,\n             'criadores': 19,\n             'atualmente': 62,\n             'tocaste': 2,\n             'cheirar': 3,\n             'trela': 1,\n             'iria': 112,\n             'alegrava': 1,\n             'significava': 23,\n             'admito': 6,\n             'acabar': 99,\n             'guerras': 29,\n             'único': 140,\n             'violência': 154,\n             'vistos': 20,\n             'precedente': 4,\n             'profeta': 4,\n             'decidiram': 33,\n             'ingredientes': 16,\n             'padarias': 3,\n             'ficou': 85,\n             'adicionar': 34,\n             'barato': 52,\n             'falantes': 9,\n             'reformarem': 3,\n             'futuro': 317,\n             'reformar': 9,\n             '25': 102,\n             'constante': 34,\n             'poupanças': 21,\n             'dezenas': 43,\n             'gps': 62,\n             'euros': 35,\n             'quantidades': 19,\n             'grandes': 237,\n             'colocá': 22,\n             'comprar': 90,\n             'vale': 43,\n             'tamanho': 163,\n             'trânsito': 42,\n             'pouco': 585,\n             'reflete': 9,\n             'estrela': 48,\n             'bloqueia': 5,\n             'planeta': 164,\n             'objeto': 51,\n             'profundidade': 17,\n             'luz': 253,\n             'transita': 1,\n             'acontecido': 31,\n             'espírito': 53,\n             'mantivéssemos': 1,\n             'estado': 241,\n             'teria': 105,\n             'instruções': 25,\n             'enganamos': 1,\n             'imensas': 23,\n             'lhes': 320,\n             'damos': 44,\n             'usada': 55,\n             'construir': 239,\n             'ferramenta': 50,\n             'destruir': 24,\n             'simples': 310,\n             'vítimas': 62,\n             'moelhores': 1,\n             'tornamos': 13,\n             'contra': 237,\n             'armas': 60,\n             'capazes': 111,\n             'cuidamos': 1,\n             'terrorismo': 41,\n             'mostrar': 301,\n             'passa': 128,\n             'fronteiras': 38,\n             'pelas': 139,\n             'lutar': 66,\n             'comunidade': 197,\n             'determinadas': 5,\n             'comunidades': 82,\n             'estávamos': 159,\n             'raparigas': 75,\n             'igreja': 23,\n             'ted': 127,\n             'otimista': 11,\n             'adoram': 10,\n             'representação': 22,\n             'crianças': 401,\n             'terrível': 46,\n             'existe': 287,\n             'autistas': 8,\n             'palavra': 166,\n             'coisa': 942,\n             'abstracta': 4,\n             'real': 206,\n             'perceber': 204,\n             'arbitrária': 2,\n             'dificuldade': 26,\n             'totalmente': 128,\n             'essencialmente': 51,\n             'nisso': 120,\n             'significa': 376,\n             'conta': 137,\n             'definição': 43,\n             'primeira': 437,\n             'salta': 7,\n             'fazê': 185,\n             'então': 2027,\n             'subir': 35,\n             'literacia': 16,\n             'vista': 129,\n             'la': 317,\n             'distrair': 3,\n             'possível': 279,\n             'sempre': 599,\n             'aliás': 18,\n             'código': 79,\n             'desta': 318,\n             'comum': 128,\n             'certo': 329,\n             'células': 179,\n             'normais': 46,\n             'método': 48,\n             'disparam': 3,\n             'padrões': 113,\n             'ajudar': 218,\n             'podíamos': 91,\n             'políticas': 68,\n             'implementar': 12,\n             'escolas': 108,\n             'boa': 223,\n             'trocar': 18,\n             'amazónia': 23,\n             'vitais': 6,\n             'pulmão': 11,\n             'gases': 12,\n             'ouvirem': 16,\n             'espacial': 44,\n             'porem': 4,\n             'mexer': 22,\n             'feriado': 2,\n             'estiverem': 54,\n             'flutuar': 9,\n             'poucos': 70,\n             'encontram': 48,\n             'momento': 385,\n             'passamos': 26,\n             'manágua': 3,\n             'percorrer': 25,\n             '250': 18,\n             'quilómetros': 57,\n             'risco': 127,\n             'guiné': 10,\n             'subido': 3,\n             'isoladas': 3,\n             'tribos': 5,\n             'semanas': 125,\n             'voltar': 153,\n             'acabado': 28,\n             'duas': 505,\n             'milénios': 9,\n             'vivendo': 5,\n             'subsistência': 4,\n             'aos': 649,\n             'planaltos': 3,\n             'papua': 7,\n             'agricultores': 45,\n             'obviamente': 96,\n             'fez': 241,\n             'filmar': 16,\n             'tocou': 6,\n             'naquele': 104,\n             'pai': 206,\n             'voraz': 1,\n             'leitor': 14,\n             'conheço': 30,\n             'terminar': 52,\n             'inglês': 79,\n             'pistas': 24,\n             'significado': 56,\n             'compreender': 147,\n             'sabia': 159,\n             'programação': 23,\n             'difícil': 339,\n             'significados': 5,\n             'trocadilhos': 2,\n             'computadores': 113,\n             'ganhar': 59,\n             'ler': 118,\n             'numa': 853,\n             'falsas': 6,\n             'jeopardy': 17,\n             'língua': 90,\n             'desvendar': 5,\n             'imaginam': 8,\n             'pista': 23,\n             'consegue': 110,\n             'duplos': 1,\n             'nestes': 63,\n             'investe': 2,\n             'natural': 130,\n             'sector': 11,\n             'privado': 31,\n             'assuntos': 22,\n             'falsa': 19,\n             'dicotomia': 4,\n             'presa': 21,\n             'meio': 288,\n             '39': 5,\n             'vir': 77,\n             'ambas': 21,\n             'conduzem': 8,\n             'extroversão': 3,\n             'agradabilidade': 1,\n             'trabalhar': 325,\n             'escolhas': 46,\n             'desconhecido': 22,\n             'desafios': 53,\n             'escuro': 37,\n             'receamos': 2,\n             'maioria': 254,\n             'alguém': 407,\n             'enfrentamos': 26,\n             'sérios': 5,\n             'mão': 172,\n             'ok': 209,\n             'problemas': 247,\n             'endémico': 1,\n             'ar': 164,\n             'dos': 2097,\n             'socialmente': 9,\n             'autoconfiantes': 2,\n             'hábeis': 5,\n             'metros': 84,\n             'avançava': 1,\n             'notícias': 112,\n             'boas': 108,\n             'diga': 44,\n             'permitam': 17,\n             'milhares': 188,\n             'grave': 31,\n             'orgânicos': 9,\n             'molécula': 38,\n             'químicos': 33,\n             'criam': 31,\n             'complicadas': 14,\n             'moléculas': 56,\n             'retrotécnica': 1,\n             'pequenas': 108,\n             'usando': 129,\n             'partindo': 2,\n             'frente': 217,\n             'tim': 11,\n             'apresentar': 46,\n             'rosling': 4,\n             'harford': 1,\n             'gráficos': 17,\n             'imaginem': 182,\n             'seus': 559,\n             'vossa': 256,\n             'terem': 94,\n             'hans': 5,\n             'fantásticos': 12,\n             'chicago': 20,\n             'resposta': 292,\n             'esqueletos': 16,\n             'cura': 28,\n             'tratamento': 66,\n             'responder': 79,\n             'matemático': 24,\n             'universais': 7,\n             'genéricos': 4,\n             'baseado': 40,\n             'modelo': 173,\n             'estas': 767,\n             'princípios': 44,\n             'questões': 116,\n             'centrais': 21,\n             'avariadas': 1,\n             'rede': 169,\n             'activas': 4,\n             'concebemos': 7,\n             'risos': 900,\n             'auto': 61,\n             'explanatório': 1,\n             ...})"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pt.word_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(1, 512)\n",
      "(1, 512)\n",
      "(50, 512)\n",
      "tf.Tensor(\n",
      "[[[ 0.          1.          0.         ...  1.          0.\n",
      "    1.        ]\n",
      "  [ 0.84147096  0.5403023   0.8218562  ...  1.          0.00010366\n",
      "    1.        ]\n",
      "  [ 0.9092974  -0.41614684  0.9364147  ...  1.          0.00020733\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.12357312 -0.9923355   0.97718984 ...  0.99998724  0.00487216\n",
      "    0.99998814]\n",
      "  [-0.76825464 -0.64014435  0.7312359  ...  0.9999867   0.00497582\n",
      "    0.9999876 ]\n",
      "  [-0.95375264  0.30059254 -0.14402692 ...  0.9999861   0.00507948\n",
      "    0.9999871 ]]], shape=(1, 50, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "终于通过代码看懂了Positional encoding是个啥\n",
    "相当于对位置信息进行embedding，假如说有一个长度为50的序列，那么对这个序列的每个位置进行编码，假如需要的维度为512\n",
    "对于其第pos个位置（例如第0个位置，pos就是0），先计算一个pos/10000^{2*i/512}的向量，i从0一直到511，所以这是一个512维度的向量\n",
    "然后如果这个pos是一个奇数，就对其计算cos，否则计算sin\n",
    "最终的position encoding就是一个50*512的向量\n",
    "\"\"\"\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  print(pos.shape)\n",
    "  print(i.shape)\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  print(angle_rates.shape)\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  print(angle_rads.shape)\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\narray([[[[0., 0., 1., 1., 0.]]],\n\n\n       [[[0., 0., 0., 1., 1.]]],\n\n\n       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 1., 1.],\n       [0., 0., 1.],\n       [0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  # 这是个有趣的api，相当于对一个矩阵斜着进行遮罩\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "  这代码写得真好，看明白了\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "  # q和k的最后两维度被当做矩阵进行相乘\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n",
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n",
      "牛逼\n"
     ]
    }
   ],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)\n",
    "  \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "\n",
    "print(\"牛逼\")\n",
    "\n",
    "# This query aligns equally with the first and second key,\n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "print(\"牛逼\")\n",
    "\n",
    "# This query aligns with a repeated key (third and fourth),\n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "print(\"牛逼\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    这个玩意的功能就比如说，输入的是(batch_size,seq_len,d_model)，然后把最后一个depth分割成(batch_size,num_heads,seq_len,depth)\n",
    "    reshape的-1代表自动计算\n",
    "    \"\"\"\n",
    "    print(x.shape)\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4  5]\n",
      "  [ 6  7  8  9 10 11]\n",
      "  [12 13 14 15 16 17]]\n",
      "\n",
      " [[18 19 20 21 22 23]\n",
      "  [24 25 26 27 28 29]\n",
      "  [30 31 32 33 34 35]]], shape=(2, 3, 6), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[[ 0  1]\n",
      "   [ 2  3]\n",
      "   [ 4  5]]\n",
      "\n",
      "  [[ 6  7]\n",
      "   [ 8  9]\n",
      "   [10 11]]\n",
      "\n",
      "  [[12 13]\n",
      "   [14 15]\n",
      "   [16 17]]]\n",
      "\n",
      "\n",
      " [[[18 19]\n",
      "   [20 21]\n",
      "   [22 23]]\n",
      "\n",
      "  [[24 25]\n",
      "   [26 27]\n",
      "   [28 29]]\n",
      "\n",
      "  [[30 31]\n",
      "   [32 33]\n",
      "   [34 35]]]], shape=(2, 3, 3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[[ 0  1]\n",
      "   [ 2  3]\n",
      "   [ 4  5]]\n",
      "\n",
      "  [[ 6  7]\n",
      "   [ 8  9]\n",
      "   [10 11]]\n",
      "\n",
      "  [[12 13]\n",
      "   [14 15]\n",
      "   [16 17]]]\n",
      "\n",
      "\n",
      " [[[18 19]\n",
      "   [20 21]\n",
      "   [22 23]]\n",
      "\n",
      "  [[24 25]\n",
      "   [26 27]\n",
      "   [28 29]]\n",
      "\n",
      "  [[30 31]\n",
      "   [32 33]\n",
      "   [34 35]]]], shape=(2, 3, 3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]]\n",
      "\n",
      "\n",
      " [[[18 19 20]\n",
      "   [21 22 23]\n",
      "   [24 25 26]]\n",
      "\n",
      "  [[27 28 29]\n",
      "   [30 31 32]\n",
      "   [33 34 35]]]], shape=(2, 2, 3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}